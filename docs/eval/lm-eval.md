# Evaluating Models with lm-eval-harness

The `lm-eval-harness` tool in `lemonade-eval` provides an easy way to evaluate language models on a variety of standardized benchmarks using the [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) framework from EleutherAI. This tool allows you to generate standardized accuracy metrics across a wide range of tasks and datasets.

## How It Works

The `lm-eval-harness` tool works with models loaded on Lemonade Server:

1. **Load Your Model**: First, load your model using the `load` tool with any model from the Lemonade Server model list (GGUF, Hybrid, NPU, etc.).

2. **Run lm-evaluation-harness**: The tool runs lm-evaluation-harness against your loaded model using the server's API.

3. **Organize Results**: Results are processed and saved to the model's build directory for easy access.

## Usage

First, start the Lemonade Server:

```bash
lemonade-server serve
```

Then run lm-eval-harness:

```bash
# With a GGUF model
lemonade-eval -i Qwen3-4B-Instruct-2507-GGUF load lm-eval-harness --task gsm8k --limit 10

# With a Hybrid model (NPU + iGPU)
lemonade-eval -i Qwen3-4B-Hybrid load lm-eval-harness --task gsm8k --limit 10

# With an NPU model
lemonade-eval -i Qwen-2.5-3B-Instruct-NPU load lm-eval-harness --task gsm8k --limit 10
```

### Common Options

- `--task`: Specifies which task to evaluate on (e.g., gsm8k, hellaswag, winogrande).
- `--limit`: Optional number of examples to evaluate (useful for quick tests).
- `--num-fewshot`: Number of examples to use in few-shot prompts (default: 0).
- `--log_samples`: Log individual samples and predictions.

## Examples

### GSM8K Math Benchmark

```bash
lemonade-eval -i Qwen3-4B-Instruct-2507-GGUF load lm-eval-harness --task gsm8k --limit 10
```

This example:
- Loads Qwen3-4B on Lemonade Server
- Evaluates on GSM8K (grade school math)
- Limits evaluation to 10 questions

### HellaSwag Commonsense Reasoning

```bash
lemonade-eval -i Qwen3-4B-Instruct-2507-GGUF load lm-eval-harness --task hellaswag --limit 50 --num-fewshot 5
```

## Supported Tasks

The tool supports many tasks available in lm-evaluation-harness. Since the Lemonade Server API does not support log probabilities, only **generation-based tasks** work with `lemonade-eval`. Recommended tasks:

- **GSM8K**: Grade School Math word problems
- **DROP**: Reading comprehension with discrete reasoning
- **TriviaQA**: Trivia question answering
- **NaturalQuestions**: Open-domain question answering

For the full list of tasks, see the [lm-evaluation-harness task list](https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks/README.md).

> **Note**: Some popular benchmarks (HellaSwag, Winogrande, ARC, MMLU, TruthfulQA) are multiple-choice tasks that require log probabilities and will not work with Lemonade Server. For MMLU evaluation, use the built-in `accuracy-mmlu` tool instead.

## Understanding Results

Results are displayed in the terminal and saved to the model's build directory.

### Metrics

The key metrics vary by task, but commonly include:

- **exact_match**: Percentage of exact matches between model predictions and expected answers.
- **acc** or **accuracy**: Accuracy score (varies by task).
- **f1**: F1 score for tasks that require partial matching.

For generative tasks like GSM8K, results often include metrics for both strict and flexible matching:

- **exact_match,strict-match**: Requires the model to produce the exact correct answer.
- **exact_match,flexible-extract**: Allows for variations in formatting but requires the correct numerical answer.

### Result Files

Detailed result files are saved in:

```
<cache_dir>/builds/<model_name>_<timestamp>/lm_eval_results/<task_name>_results/
```

These include the full evaluation data in JSON format.

## Interpreting Results

When evaluating models, consider:

1. **Task Relevance**: Different tasks measure different capabilities. Choose tasks relevant to your use case.

2. **Comparison Context**: Compare results against other models of similar size/architecture for meaningful insights.

3. **Few-shot Performance**: Many models perform significantly better with examples (try `--num-fewshot 5`).

4. **Limitations**: Low scores on specific tasks may highlight limitations in the model's training data or capabilities.

Summary `lm-eval-harness` tool results are also included in the tables generated by
the report tool (`lemonade-eval report --perf`).

## Further Information

For more details on lm-evaluation-harness and its capabilities, see the [official documentation](https://github.com/EleutherAI/lm-evaluation-harness).

<!--This file was originally licensed under Apache 2.0. It has been modified.
Modifications Copyright (c) 2025 AMD-->
